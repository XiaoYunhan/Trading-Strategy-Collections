{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S&P 500 Index Changes Data Processing\n",
    "\n",
    "This notebook processes HTML data from Wikipedia to extract S&P 500 component changes with announcement dates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Create data folder if it doesn't exist\n",
    "os.makedirs('data', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Parse Component Changes from HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 375 component changes\n"
     ]
    }
   ],
   "source": [
    "# Read the HTML file\n",
    "with open('./data/ComponentUpdate.html', 'r', encoding='utf-8') as file:\n",
    "    html_content = file.read()\n",
    "\n",
    "# Parse HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "# Find the table\n",
    "table = soup.find('table', {'id': 'changes'})\n",
    "\n",
    "# Extract data from table\n",
    "data = []\n",
    "rows = table.find('tbody').find_all('tr')\n",
    "\n",
    "for row in rows:\n",
    "    cells = row.find_all('td')\n",
    "    if len(cells) >= 5:\n",
    "        date = cells[0].get_text(strip=True)\n",
    "        added_ticker = cells[1].get_text(strip=True)\n",
    "        added_security = cells[2].get_text(strip=True)\n",
    "        removed_ticker = cells[3].get_text(strip=True)\n",
    "        removed_security = cells[4].get_text(strip=True)\n",
    "        reason_cell = cells[5] if len(cells) > 5 else None\n",
    "        \n",
    "        # Extract reference numbers\n",
    "        reference_numbers = []\n",
    "        if reason_cell:\n",
    "            sup_tags = reason_cell.find_all('sup', class_='reference')\n",
    "            for sup in sup_tags:\n",
    "                cite_id = sup.get('id', '')\n",
    "                if cite_id:\n",
    "                    match = re.search(r'cite_ref-(?:[^_]+_)?(\\d+)', cite_id)\n",
    "                    if match:\n",
    "                        reference_numbers.append(match.group(1))\n",
    "        \n",
    "        # Get clean reason text\n",
    "        reason = cells[5].get_text(strip=True) if len(cells) > 5 else ''\n",
    "        reason = re.sub(r'\\[\\d+\\]', '', reason)\n",
    "        \n",
    "        data.append({\n",
    "            'Effective_Date': date,\n",
    "            'Added_Ticker': added_ticker,\n",
    "            'Added_Security': added_security,\n",
    "            'Removed_Ticker': removed_ticker,\n",
    "            'Removed_Security': removed_security,\n",
    "            'Reason': reason.strip(),\n",
    "            'Reference_Numbers': ','.join(reference_numbers) if reference_numbers else ''\n",
    "        })\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert Date column to datetime\n",
    "df['Effective_Date'] = pd.to_datetime(df['Effective_Date'], format='%B %d, %Y', errors='coerce')\n",
    "\n",
    "print(f\"Extracted {len(df)} component changes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Extract Announcement Dates from References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted dates from 266 references\n"
     ]
    }
   ],
   "source": [
    "# Read the Reference.html file\n",
    "with open('./data/Reference.html', 'r', encoding='utf-8') as file:\n",
    "    ref_html_content = file.read()\n",
    "\n",
    "ref_soup = BeautifulSoup(ref_html_content, 'html.parser')\n",
    "\n",
    "def extract_all_dates_from_text(text):\n",
    "    \"\"\"Extract all dates from text using multiple patterns.\"\"\"\n",
    "    dates_found = []\n",
    "    \n",
    "    # Clean the text\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "    \n",
    "    # Pattern 1: YYYY-MM-DD format\n",
    "    pattern1 = r'(\\d{4}-\\d{2}-\\d{2})'\n",
    "    for match in re.findall(pattern1, text):\n",
    "        try:\n",
    "            dates_found.append(datetime.strptime(match, '%Y-%m-%d'))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Pattern 2: Month DD, YYYY format\n",
    "    pattern2 = r'((?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2},?\\s+\\d{4})'\n",
    "    for match in re.findall(pattern2, text):\n",
    "        try:\n",
    "            clean_match = re.sub(r',', '', match)\n",
    "            dates_found.append(datetime.strptime(clean_match, '%B %d %Y'))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    # Pattern 3: YYYYMMDD in URLs\n",
    "    pattern3 = r'/(\\d{8})-'\n",
    "    for match in re.findall(pattern3, text):\n",
    "        try:\n",
    "            dates_found.append(datetime.strptime(match, '%Y%m%d'))\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "    return dates_found\n",
    "\n",
    "# Extract references with their dates\n",
    "reference_dates = {}\n",
    "all_references = ref_soup.find_all('li', id=re.compile(r'cite_note-'))\n",
    "\n",
    "for ref_item in all_references:\n",
    "    ref_id = ref_item.get('id', '')\n",
    "    ref_num_match = re.search(r'cite_note-(?:.*?-)?(\\d+)$', ref_id)\n",
    "    \n",
    "    if ref_num_match:\n",
    "        ref_num = ref_num_match.group(1)\n",
    "        ref_text = ref_item.get_text()\n",
    "        dates = extract_all_dates_from_text(ref_text)\n",
    "        \n",
    "        if dates:\n",
    "            # Get the earliest date (most relevant for trading)\n",
    "            reference_dates[ref_num] = min(dates)\n",
    "\n",
    "print(f\"Extracted dates from {len(reference_dates)} references\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Merge Announcement Dates and Create Final Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Dataset saved to data/sp500_index_changes.csv\n",
      "Total records: 375\n",
      "Records with announcement dates: 342\n",
      "\n",
      "First 5 records:\n",
      "  Announcement_Date Effective_Date Added_Ticker    Added_Security  \\\n",
      "0        2025-07-18     2025-07-23          XYZ       Block, Inc.   \n",
      "1        2025-07-14     2025-07-18          TTD  Trade Desk (The)   \n",
      "2        2025-07-02     2025-07-09         DDOG           Datadog   \n",
      "3        2025-05-12     2025-05-19         COIN          Coinbase   \n",
      "4        2025-03-07     2025-03-24         DASH          DoorDash   \n",
      "\n",
      "  Removed_Ticker    Removed_Security  \\\n",
      "0            HES    Hess Corporation   \n",
      "1           ANSS               Ansys   \n",
      "2           JNPR    Juniper Networks   \n",
      "3            DFS  Discover Financial   \n",
      "4            BWA          BorgWarner   \n",
      "\n",
      "                                              Reason  \n",
      "0  S&P 500 and S&P 100 constituent Chevron Corp. ...  \n",
      "1  S&P 500 constituent Synopsys Inc. acquired Ansys.  \n",
      "2  S&P 500 constituent Hewlett Packard Enterprise...  \n",
      "3  S&P 500 constituent Capital One Financial Corp...  \n",
      "4                      Market capitalization change.  \n"
     ]
    }
   ],
   "source": [
    "# Add announcement dates to the main dataframe\n",
    "df['Announcement_Date'] = pd.NaT\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    ref_nums = str(row['Reference_Numbers']).split(',')\n",
    "    ref_nums = [num.strip() for num in ref_nums if num.strip()]\n",
    "    \n",
    "    if ref_nums:\n",
    "        dates = []\n",
    "        for ref_num in ref_nums:\n",
    "            if ref_num in reference_dates:\n",
    "                dates.append(reference_dates[ref_num])\n",
    "        \n",
    "        if dates:\n",
    "            df.at[idx, 'Announcement_Date'] = min(dates)\n",
    "\n",
    "# Select only required columns in specified order\n",
    "final_df = df[['Announcement_Date', 'Effective_Date', 'Added_Ticker', 'Added_Security', \n",
    "               'Removed_Ticker', 'Removed_Security', 'Reason']]\n",
    "\n",
    "# Sort by Effective_Date (most recent first)\n",
    "final_df = final_df.sort_values('Effective_Date', ascending=False)\n",
    "\n",
    "# Save to data folder\n",
    "output_path = 'data/sp500_index_changes.csv'\n",
    "final_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"\\n✅ Dataset saved to {output_path}\")\n",
    "print(f\"Total records: {len(final_df)}\")\n",
    "print(f\"Records with announcement dates: {final_df['Announcement_Date'].notna().sum()}\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nFirst 5 records:\")\n",
    "print(final_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
